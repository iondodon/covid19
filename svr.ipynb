{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "import math\n",
    "import seaborn as sns\n",
    "import impyute as impy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(s):\n",
    "    return datetime.strptime(s, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = pd.read_csv('cleaned_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cloumn Unnamed: 0\n",
    "\n",
    "# Fixing date column datatype\n",
    "cleaned_dataset['date'] = pd.to_datetime(cleaned_dataset['date']);\n",
    "# all_countries_dataset['location'] = all_countries_dataset['location'].astype('category');\n",
    "# all_countries_dataset['continent'] = all_countries_dataset['continent'].astype('category');\n",
    "\n",
    "# all_countries_dataset.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "continent                    object\n",
       "location                     object\n",
       "date                 datetime64[ns]\n",
       "new_cases                   float64\n",
       "new_deaths                  float64\n",
       "icu_patients                float64\n",
       "new_tests                   float64\n",
       "positive_rate               float64\n",
       "people_vaccinated           float64\n",
       "new_vaccinations            float64\n",
       "total_boosters              float64\n",
       "stringency_index            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take only numerical variables\n",
    "numerical_variables = [\n",
    "    'new_cases',\n",
    "    'new_deaths',\n",
    "    'icu_patients',\n",
    "    'new_tests',\n",
    "    'positive_rate',\n",
    "    'people_vaccinated',\n",
    "    'new_vaccinations',\n",
    "    'total_boosters',\n",
    "    'stringency_index'\n",
    "]\n",
    "\n",
    "# take non-numerical variables\n",
    "non_numerical_variables = [\n",
    "    'continent',\n",
    "    'location',\n",
    "    'date'\n",
    "]\n",
    "\n",
    "svr_dataset = cleaned_dataset.copy()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "obj_dataset = svr_dataset[non_numerical_variables].copy()\n",
    "num_dataset = svr_dataset[numerical_variables].copy()\n",
    "# num_dataset = sc.fit_transform(num_dataset)\n",
    "\n",
    "num_dataset = pd.DataFrame(num_dataset, columns=numerical_variables)\n",
    "\n",
    "\n",
    "num_dataset = np.log(num_dataset)\n",
    "\n",
    "# Take First Difference to Remove Trend\n",
    "num_dataset = num_dataset.diff()\n",
    "\n",
    "num_dataset = num_dataset.diff()\n",
    "\n",
    "# Remove Increasing Volatility\n",
    "# num_dataset = num_dataset.groupby(num_dataset.index.year).std()\n",
    "\n",
    "svr_dataset = pd.concat([obj_dataset, pd.DataFrame(data=num_dataset, columns=numerical_variables)], axis=1)\n",
    "\n",
    "svr_dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_dataset.index = pd.to_datetime(svr_dataset.date)\n",
    "svr_dataset.index.freq = svr_dataset.index.inferred_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nans(dataset):\n",
    "    # for each country\n",
    "    new_dataset = pd.DataFrame()\n",
    "\n",
    "    for country in dataset['location'].unique():\n",
    "        country_dataset = dataset[dataset['location'] == country]\n",
    "\n",
    "        for variable in numerical_variables:\n",
    "            for index, row in country_dataset.iterrows():\n",
    "                if pd.isna(row[variable]) or row[variable] == float('inf') or row[variable] == float('-inf'):\n",
    "                    previous_timestamp = index - pd.Timedelta(days=1)\n",
    "                    if previous_timestamp in country_dataset.index and pd.isna(country_dataset.loc[previous_timestamp, variable]) == False:\n",
    "                        country_dataset.at[index, variable] = country_dataset.at[previous_timestamp, variable]\n",
    "                    else:\n",
    "                        country_dataset.at[index, variable] = 0\n",
    "    \n",
    "        new_dataset = pd.concat([new_dataset, country_dataset], axis=0)\n",
    "\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_dataset = replace_nans(svr_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_lags_dependent = 1 # calculated with AutoARIMA\n",
    "nb_lags_independent = 1 # calculated with AutoARIMA\n",
    "\n",
    "vars_to_lag = [\n",
    "    'icu_patients',\n",
    "    'new_tests',\n",
    "    'new_vaccinations'\n",
    "]\n",
    "\n",
    "# for each variable\n",
    "for variable in vars_to_lag:\n",
    "    if variable == 'new_deaths':\n",
    "        continue\n",
    "    for nb_lag in range(0, nb_lags_independent):\n",
    "        svr_dataset[variable + '_lag' + str(nb_lag)] = svr_dataset[variable].shift(nb_lag)\n",
    "\n",
    "for nb_lag in range(nb_lags_dependent):\n",
    "    svr_dataset['new_deaths_lag' + str(nb_lag)] = svr_dataset['new_deaths'].shift(nb_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nan values created by shift()\n",
    "svr_dataset = svr_dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non-lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['continent', 'location', 'date', 'new_cases', 'new_deaths', 'icu_patients', 'new_tests', 'positive_rate', 'people_vaccinated', 'new_vaccinations', 'total_boosters', 'stringency_index', 'icu_patients_lag0', 'new_tests_lag0', 'new_vaccinations_lag0', 'new_deaths_lag0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['icu_patients_lag0',\n",
       " 'new_tests_lag0',\n",
       " 'new_vaccinations_lag0',\n",
       " 'new_deaths_lag0',\n",
       " 'new_deaths']"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(svr_dataset.columns.tolist())\n",
    "predictors = svr_dataset.columns.tolist()\n",
    "# leave only those that contain '_lag'\n",
    "predictors = [x for x in predictors if '_lag' in x]\n",
    "# add date and new_deaths\n",
    "predictors.append('new_deaths')\n",
    "\n",
    "predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = svr_dataset['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ion/.local/lib/python3.9/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "# for each country\n",
    "for country in svr_dataset['location'].unique():\n",
    "    # set empty column with 0 values for svr_dataset[country]\n",
    "    predictors.append(country)\n",
    "    svr_dataset[country] = 0\n",
    "    svr_dataset[country].loc[svr_dataset['location'] == country] = 1\n",
    "\n",
    "# drop location column\n",
    "# svr_dataset = svr_dataset.drop(columns=['location'])\n",
    "# drop continent column\n",
    "# svr_dataset = svr_dataset.drop(columns=['continent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-06 00:00:00\n",
      "2021-11-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# find max date\n",
    "max_date = svr_dataset['date'].max()\n",
    "min_date = svr_dataset['date'].min()\n",
    "print(min_date)\n",
    "print(max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "def svr(svr_train_dataset):\n",
    "    y = svr_train_dataset['new_deaths']\n",
    "\n",
    "    pp = [x for x in predictors if x != 'new_deaths']\n",
    "    X = svr_train_dataset[pp]\n",
    "\n",
    "    svr_model = SVR()\n",
    "    svr_model.fit(X, y)\n",
    "\n",
    "    return svr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svr_predict(model, country, svr_test_dataset, plot):\n",
    "    y_test = svr_test_dataset['new_deaths'].loc[svr_test_dataset[country] == 1]\n",
    "\n",
    "    pp = [x for x in predictors if x != 'new_deaths']\n",
    "    X_test = svr_test_dataset[pp].loc[svr_test_dataset[country] == 1]\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    print(\"Country: {} - C: {} - SVR Score: {}\".format(country, model.C, model.score(X_test, y_test)))\n",
    " \n",
    "    if plot:\n",
    "        # plot predictions vs actual\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.plot(svr_test_dataset['date'].loc[svr_test_dataset[country] == 1], predictions, label='Predictions')\n",
    "        plt.plot(svr_test_dataset['date'].loc[svr_test_dataset[country] == 1], y_test, label='Actual')\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"New Deaths\")\n",
    "        plt.title(country)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def grid_search(country):\n",
    "    # split into train and test\n",
    "    training_date_limit = date(2021, 10, 15)\n",
    "\n",
    "    svr_dataset.index = pd.to_datetime(svr_dataset.date)\n",
    "    svr_dataset.index.freq = svr_dataset.index.inferred_freq\n",
    "\n",
    "    svr_train_dataset = svr_dataset[svr_dataset['date'].dt.date < training_date_limit]\n",
    "    svr_test_dataset = svr_dataset[svr_dataset['date'].dt.date >= training_date_limit]\n",
    "\n",
    "    svr_model = svr(svr_train_dataset)\n",
    "\n",
    "    y = svr_test_dataset['new_deaths'].loc[svr_test_dataset[country] == 1]\n",
    "    \n",
    "    pp = [x for x in predictors if x != 'new_deaths']\n",
    "    X = svr_test_dataset[pp].loc[svr_test_dataset[country] == 1]\n",
    "    \n",
    "    parameters = {\n",
    "        'C': [0.01, 0.1, 0.2, 0.5, 1, 10, 50, 100, 1000],\n",
    "        'gamma': [0.001, 0.0001],\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'epsilon': [0.0001, 0.001, 0.0001, 0.00001]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(svr_model, parameters)\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.5, 'epsilon': 1e-05, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "0.9999998486377148\n"
     ]
    }
   ],
   "source": [
    "gs = grid_search('United States')\n",
    "\n",
    "# show best parameters\n",
    "print(gs.best_params_)\n",
    "\n",
    "# show best score\n",
    "print(gs.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation():\n",
    "    index = min_date + pd.Timedelta(days=120)\n",
    "\n",
    "    while index <= max_date:\n",
    "        print(\"=========================================================== Data taken from {} to {}\".format(min_date, index))\n",
    "\n",
    "        # get data where svr_dataset.index < index\n",
    "        df = svr_dataset[svr_dataset.index < index]\n",
    "        df.index = pd.to_datetime(df.date)\n",
    "        print(df.shape)\n",
    "\n",
    "        # 80% of the number of days from min_date to index\n",
    "        training_date_limit = min_date + pd.Timedelta(days=int(0.8 * (index - min_date).days))\n",
    "\n",
    "        svr_train_dataset = df[df.index < training_date_limit]\n",
    "        svr_test_dataset = df[df.index >= training_date_limit]\n",
    "\n",
    "        model = svr(svr_train_dataset)\n",
    "        # for each country\n",
    "        for country in countries:\n",
    "            plot = False\n",
    "            if index + pd.Timedelta(days=120) > max_date:\n",
    "                plot = True\n",
    "            svr_predict(model, country, svr_test_dataset, plot)\n",
    "\n",
    "        index = index + pd.Timedelta(days=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validation()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
